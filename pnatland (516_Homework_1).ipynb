{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIADS 516: Homework 1\n",
    "Version 1.0.20200221.1\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first mrjob script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the following example from the lectures:\n",
    "\n",
    "Note the use of the magic command ```%%file```.  You can use this to write the contents of a cell out to a file, which is what we need to do to use mrjob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    yield \"chars\", len(line)\n",
    "    yield \"words\", len(line.split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    yield key, sum(values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the output code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chars', 70), ('words', 12), ('lines', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[('chars', 1), ('words', 0), ('lines', 1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code testing out the output of mapper:\n",
    "def read_file(filename):\n",
    "    \"\"\"Read the text file (filename) and return a list of the lines from it\"\"\"\n",
    "    fp = open(filename)\n",
    "    L = fp.readlines()\n",
    "    return L\n",
    "\n",
    "def mapper_try(filename, line_num):\n",
    "    line_list = read_file(filename)\n",
    "    yield \"chars\", len(line_list[line_num])\n",
    "    yield \"words\", len(line_list[line_num].split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "display(list(mapper_try('data/gutenberg/short.t1.txt', 0)), \n",
    "        list(mapper_try('data/gutenberg/short.t1.txt', 1))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trying out logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count_logger.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count_logger.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "import logging\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    logging.info(f'  line length: {len(line)}')\n",
    "    yield \"chars\", len(line)\n",
    "    logging.info(f'  splt length: {len(line.split())}')\n",
    "    yield \"words\", len(line.split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    logging.info(f'  word total: {key, sum(values)}')\n",
    "    yield key, sum(values)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(filename='wordfreq.log', level=logging.INFO)\n",
    "    logging.info('Started')\n",
    "    MRWordFrequencyCount.run()\n",
    "    logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count_logger.jovyan.20200707.184948.251923\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count_logger.jovyan.20200707.184948.251923/output\n",
      "Streaming final output from /tmp/word_count_logger.jovyan.20200707.184948.251923/output...\n",
      "\"chars\"\t0\n",
      "\"words\"\t0\n",
      "\"lines\"\t0\n",
      "Removing temp directory /tmp/word_count_logger.jovyan.20200707.184948.251923...\n"
     ]
    }
   ],
   "source": [
    "!python word_count_logger.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q1: Explain what each of the yield statements in the above script do.  Provide a list of what the first few iterations through the mapper() step would yield if the script was run against the ```data/gutenberg/short.t1.txt``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mapper will go through each line of the input text. ```yield \"chars\", len(line)``` will count the number of characters present in the line (including spaces).  ```yield \"words\", len(line.split())``` will string-split the line at each space and will count the number words, etc. that are separated by spaces.  ```yield \"lines\", 1``` will always return the world \"lines\" and the number 1.\n",
    "\n",
    "> Each time you call the mapper generator it will return the next yield and will cycle back.  I would expect the first few iterations would yield the following:\n",
    "\n",
    "> #this is for the first text line as determined by __logging.info__ (the first three iterations) <br>\n",
    "    >(\"chars\", 69) <br>\n",
    "    >(\"words\", 12) <br>\n",
    "    >(\"lines\", 1)  <br>\n",
    "> #this is for the second line -- which is empty as determined by __logging.info__ (the next three iterations) <br>\n",
    "    >(\"chars\", 0) <br>\n",
    "    >(\"words\", 0) <br>\n",
    "    >(\"lines\", 1) <br>\n",
    "    \n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the output of running the script against that same file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20200707.184948.761482\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20200707.184948.761482/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20200707.184948.761482/output...\n",
      "\"chars\"\t10653\n",
      "\"words\"\t1822\n",
      "\"lines\"\t200\n",
      "Removing temp directory /tmp/word_count.jovyan.20200707.184948.761482...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2.  Repeat the above cell using the the works of William Shakespeare text file (data/gutenberg/t8.shakespeare.txt).  Provide an interpretation of the output (don't overthink this -- just demonstrate that you can find the relevant information in the output).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20200707.184949.236212\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20200707.184949.236212/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20200707.184949.236212/output...\n",
      "\"chars\"\t5333743\n",
      "\"words\"\t901325\n",
      "\"lines\"\t124456\n",
      "Removing temp directory /tmp/word_count.jovyan.20200707.184949.236212...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A temporary directory with a randomly generated name is created (and is removed after commands are executed).  \n",
    "> __The output we want is:__<br>\n",
    "> ```\"chars\" 5333743\n",
    "> \"words\" 901325\n",
    "> \"lines\" 124456```<br>\n",
    "> The \"chars\" value represents the number of characters present in the whole Shakespeare textfile, and the \"words\" and \"lines\" represent the number of words and lines in the whole textfile, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at a slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this yields the output of the mapper...couldn't get to work with combiner or reducer...\n",
    "\n",
    "# !python most_used_word.py --step-num=0 --mapper < data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working on logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word_logger.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word_logger.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "import logging\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                logging.info(f'  words: {(word.lower(), 1)}')\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        logging.info(f'  combiner word_counts: {(word, sum(counts))}')\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        logging.info(f'  reducer1 word_counts: {(sum(counts), word)}')\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        output = max(word_count_pairs)\n",
    "        logging.info(f'  reducer2 most_used_word: {output}')\n",
    "        yield output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logging.basicConfig(filename='mostusedwords.log', level=logging.INFO)\n",
    "    logging.info('Started')\n",
    "    MRMostUsedWord.run()\n",
    "    logging.info('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word_logger.jovyan.20200707.184951.906882\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word_logger.jovyan.20200707.184951.906882/output\n",
      "Streaming final output from /tmp/most_used_word_logger.jovyan.20200707.184951.906882/output...\n",
      "0\t\"yesterday\"\n",
      "Removing temp directory /tmp/most_used_word_logger.jovyan.20200707.184951.906882...\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word_logger.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing some of the the output code (outside MRJob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "STOPWORDS = ['i', 'we', 'ourselves', 'hers', 'between', 'yourself', \n",
    "             'but', 'again', 'there', 'about', 'once', 'during', 'out', \n",
    "             'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', \n",
    "             'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', \n",
    "             'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', \n",
    "             'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', \n",
    "             'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', \n",
    "             'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', \n",
    "             'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "             'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', \n",
    "             'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', \n",
    "             'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', \n",
    "             'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'\n",
    "            ]\n",
    "\n",
    "def mapper_get_words_try(filename, line_num):\n",
    "    # yield each word in the line\n",
    "\n",
    "    WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "    line_list = read_file(filename)\n",
    "    for word in WORD_RE.findall(line_list[line_num]):\n",
    "        if word.lower() not in STOPWORDS:\n",
    "            yield (word.lower(), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('project', 1),\n",
       " (\"gutenberg's\", 1),\n",
       " ('year', 1),\n",
       " ('2889', 1),\n",
       " ('jules', 1),\n",
       " ('verne', 1),\n",
       " ('michel', 1),\n",
       " ('verne', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mapper_get_words_try('data/gutenberg/short.t1.txt', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q3: Explain what the yield statements in the  above script do.  Provide a list of what the first few iterations through the steps would yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - __mapper_get_words__ will extract each word from a line of text and return an tuple consisting of each word and the the count \"1\".  If a word is repeated, it will not combine them.<br>\n",
    ">   - _below are the results from the first several iterations as determined by __logging.info__:_ <br>\n",
    "         > ('project', 1)<br>\n",
    "         > (\"gutenberg's\", 1)<br>\n",
    "         > ('year', 1)<br>\n",
    "         > ('2889', 1)<br>\n",
    "         > ('jules', 1)<br>\n",
    "         > ('verne', 1)<br>\n",
    "         > ('michel', 1) <br>\n",
    "         > ('verne', 1)<br>\n",
    "         > ('ebook', 1)<br>\n",
    "         > ('use', 1)<br>\n",
    "         > ('anyone', 1)<br>\n",
    "         > ('anywhere', 1)<br>\n",
    "         > ('cost', 1)<br>\n",
    "         > ('almost', 1)<br>\n",
    "         > ('restrictions', 1)<br>\n",
    "         > ('whatsoever', 1)<br>\n",
    "         > ('may', 1)<br>\n",
    "         > ('copy', 1)<br>\n",
    "         > ('give', 1)<br>\n",
    "         > ('away', 1)<br>\n",
    "<br>\n",
    "> - __combiner_count_words__ will shuffle together common words and combine the counts so, for example, if a word is repeated (as in line 10) it will yield the word and its total count up to that point, e.g. ('verne', 2).\n",
    ">   - _below are the results from the first several iterations as determined by __logging.info__:_ <br>\n",
    "         > ('2889', 1)<br>\n",
    "         > ('almost', 1)<br>\n",
    "         > ('anyone', 1)<br> \n",
    "         > ('anywhere', 1)<br> \n",
    "         > ('away', 1)<br>\n",
    "         > ('copy', 1)<br>\n",
    "         > ('cost', 1)<br>\n",
    "         > ('ebook', 1)<br>\n",
    "         > ('give', 1)<br>\n",
    "         > (\"gutenberg's\", 1)<br>\n",
    "         > ('jules', 1)<br>\n",
    "         > ('may', 1)<br>\n",
    "         > ('michel', 1)<br>\n",
    "         > ('project', 1)<br>\n",
    "         > ('restrictions', 1)<br>\n",
    "         > ('use', 1)<br>\n",
    "         > ('verne', 2)<br>\n",
    "         > ('whatsoever', 1)<br> \n",
    "         > ('year', 1)<br>\n",
    "<br>         \n",
    "> - __reducer_count_words__ will perform the same sum but reorders the tuple that is yielded by the prior generator, numerical occurrence first.\n",
    ">   - _below are the results from the first several iterations as determined by __logging.info__:_ <br>\n",
    "         > (7, '000')<br>\n",
    "         > (2, '10')<br>\n",
    "         > (2, '1000')<br>\n",
    "         > (1, '1100')<br>\n",
    "         > (1, '1889')<br>\n",
    "         > (1, '19362')<br>\n",
    "         > (1, '2')<br>\n",
    "         > (1, '200')<br>\n",
    "         > (1, '2006')<br>\n",
    "         > (1, '2007')<br>\n",
    "         > (1, '23')<br>\n",
    "         > (1, '250')<br>\n",
    "         > (1, '253d')<br>\n",
    "         > (1, '25th')<br>\n",
    "         > (1, '262')<br>\n",
    "         > (1, '2792')<br>\n",
    "         > (6, '2889')<br>\n",
    "         > (1, '2889_')<br>\n",
    "         > (1, '3')<br>\n",
    "<br>         \n",
    "> - __reducer_find_max_word__ will yield a tuple with the word that is used the most times in a text and the count of the number of times it was used (with the numerical occurrence being the first element of the tuple).\n",
    ">   - _below are the results as determined by __logging.info__:_ <br>\n",
    "         > (11, 'day')<br>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the file against data/gutenberg/short.t1.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20200707.184952.966224\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20200707.184952.966224/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20200707.184952.966224/output...\n",
      "11\t\"day\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20200707.184952.966224...\n",
      "0.7241528034210205\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20200707.184953.915087\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20200707.184953.915087/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20200707.184953.915087/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20200707.184953.915087...\n",
      "4.554636001586914\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: Run the above script on the Shakespeare text file.  What answer do you get?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above code indicates that the most common word in the Shakespeare text is \"thou\" and it was used 5479 times.  After closing the temporary directory it also prints how long it takes to run __MRMostUsedWord__ by recording the time right before and right after running the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking impact of removing the combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word2.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word2.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "#                    combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "                \n",
    "#     def combiner_count_words(self, word, counts):\n",
    "#         # optimization: sum the words we've seen so far\n",
    "#         yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word2.jovyan.20200707.184958.757899\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word2.jovyan.20200707.184958.757899/output\n",
      "Streaming final output from /tmp/most_used_word2.jovyan.20200707.184958.757899/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word2.jovyan.20200707.184958.757899...\n",
      "3.7420153617858887\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word2.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q5: What is the impact of removing the combiner from the above code in terms of efficiency?  What does that suggest?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It takes less time to run the code without the \"combiner\" generator.  This appears to be true because the reducer code already has __sum(counts)__ in it so the reducer is already performing the tast of the combiner, thus the combiner makes the process take longer with the data input (in this case, one text file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q6: Write an mrjob script that finds the 10 words that have the most syllables from the t5.churchill.txt file.  Interpret your results.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### installs & imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "Requirement already satisfied: syllapy in /opt/conda/lib/python3.7/site-packages (0.7.1)\n",
      "Requirement already satisfied: ujson<2.0,>=1.35 in /opt/conda/lib/python3.7/site-packages (from syllapy) (1.35)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install syllapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllapy\n",
    "    #to count syllables\n",
    "\n",
    "import sys\n",
    "    # sys.exit() allows us to quit (if we can't read a file)\n",
    "\n",
    "from functools import lru_cache\n",
    "    #to use the @lru_cache decorator\n",
    "    \n",
    "from operator import itemgetter\n",
    "    #to help with more efficient sorting\n",
    "    \n",
    "import re\n",
    "    #to apply regular expressions to extract words from text (in my read_file function)\n",
    "\n",
    "from collections import defaultdict\n",
    "    #for better dictionary functionality (in my read_file function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First: creating code/functions that returns the words with the most syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#got this from a SIADS 515 assignment of mine (HW4)\n",
    "\n",
    "@lru_cache(128)\n",
    "def read_file(filename):\n",
    "    \"\"\"Read the text file (filename) and return a dictionary (defaultdict) with unique words and their frequency.\n",
    "    \n",
    "    Function uses regex to extract all of the words in each line of filename after converting words to lower-case.\n",
    "    It then stores the words as keys and counts their frequency in the defaultdict.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        wordfreq = defaultdict(int)\n",
    "        with open(filename, 'r') as file:\n",
    "            for line in file:\n",
    "                for word in re.findall(r'[A-Za-z0-9]+', line.lower()):\n",
    "                    wordfreq[word]+=1\n",
    "        \n",
    "    except IOError as excObj:\n",
    "        print(str(excObj))\n",
    "        print(\"Error opening or reading input file: \" + filename)\n",
    "        sys.exit()\n",
    "        \n",
    "    return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(128)\n",
    "def top_n_syllables(filename, n):\n",
    "    \n",
    "    word_dict = read_file(filename)  \n",
    "    \n",
    "    #retrieve just the keys (the unique words in the textfile)\n",
    "    word_list = list(word_dict.keys())\n",
    "    \n",
    "    word_list.sort(key=len, reverse=True)\n",
    "    \n",
    "    def syllables_list(word_list):\n",
    "        #EXCEPTIONS: words that contain the parts in the lists below are frequently miscounted by syllapy\n",
    "        undercount_word_part = ['bio', 'ia', 'ism']\n",
    "        overcount_word_part = ['ate', 'ive', 'ize']\n",
    "#         l = []\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                #for undercounted words\n",
    "                if any(word in word_list[i] for word in undercount_word_part):\n",
    "                    yield (word_list[i], syllapy.count(word_list[i])+1) \n",
    "               \n",
    "                #for overcountered words\n",
    "                elif any(word in word_list[i] for word in overcount_word_part):\n",
    "                    yield (word_list[i], syllapy.count(word_list[i])-1)\n",
    "                \n",
    "                 # this was an easy way to check other word parts (e.g. suffixes) to check accuracy                 \n",
    "#                elif any(word in word_list[i] for word in l):\n",
    "#                     print((word_list[i], syllapy.count(word_list[i])))\n",
    "                \n",
    "                #for the rest!    \n",
    "                else:\n",
    "                    yield (word_list[i], syllapy.count(word_list[i]))\n",
    "                i += 1\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    #converts generator object produced into a list\n",
    "    syllables_list = list(syllables_list(word_list))\n",
    "    \n",
    "    #first, sort alphabetically so it is retained within next sort\n",
    "    syllables_list.sort()\n",
    "    \n",
    "    #sort the list by the second value within the tuple, the syllable number\n",
    "    syllables_list.sort(key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    return syllables_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(128)\n",
    "def top_n_syllables(filename, n):\n",
    "    \n",
    "    word_dict = read_file(filename)  \n",
    "    #retrieve just the keys (the unique words in the textfile)\n",
    "    word_list = list(word_dict.keys())\n",
    "    word_list.sort(key=len, reverse=True)\n",
    "    \n",
    "    def syllables_list(word_list):\n",
    "        #EXCEPTIONS: words that contain the parts in the lists below are frequently miscounted by syllapy\n",
    "        undercount_word_part = ['bio', 'ia', 'ism']\n",
    "        overcount_word_part = ['ate', 'ive', 'ize']\n",
    "\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                #for undercounted words\n",
    "                if any(word in word_list[i] for word in undercount_word_part):\n",
    "                    yield (word_list[i], syllapy.count(word_list[i])+1) \n",
    "               \n",
    "                #for overcountered words\n",
    "                elif any(word in word_list[i] for word in overcount_word_part):\n",
    "                    yield (word_list[i], syllapy.count(word_list[i])-1)\n",
    "                \n",
    "                #for the rest!    \n",
    "                else:\n",
    "                    yield (word_list[i], syllapy.count(word_list[i]))\n",
    "                i += 1\n",
    "            except:\n",
    "                break\n",
    "    \n",
    "    #converts generator object produced into a list\n",
    "    syllables_list = list(syllables_list(word_list))\n",
    "    \n",
    "    #first, sort alphabetically so it is retained within next sort\n",
    "    syllables_list.sort()\n",
    "    \n",
    "    #sort the list by the second value within the tuple, the syllable number\n",
    "    syllables_list.sort(key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    return syllables_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 ns ± 0.242 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit top_n_syllables('data/gutenberg/t5.churchill.txt', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interesting: much faster than MRJob...but task is also fairly simple, I suppose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('incommunicability', 8),\n",
       " ('materialistically', 8),\n",
       " ('overcapitalization', 8),\n",
       " ('apologetically', 7),\n",
       " ('appreciatively', 7),\n",
       " ('artificiality', 7),\n",
       " ('autobiographical', 7),\n",
       " ('characteristically', 7),\n",
       " ('cosmopolitanism', 7),\n",
       " ('enthusiastically', 7)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n_syllables('data/gutenberg/t5.churchill.txt', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting syllables_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file syllables_count.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "import syllapy\n",
    "from operator import itemgetter\n",
    "    #to help with more efficient sorting\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "class MRWordSyllableCounter(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_syllables),\n",
    "            MRStep(reducer=self.reducer_syllable_sorter)\n",
    "               ]\n",
    "    \n",
    "    ### input: self, in_key, in_value\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "            \n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_syllables(self, word, _):\n",
    "        syllables = syllapy.count(word)\n",
    "        #EXCEPTIONS: words that contain these parts are frequently miscounted by syllapy\n",
    "        undercount_word_part = ['bio', 'ia', 'ism']\n",
    "        overcount_word_part = ['ate', 'ive', 'ize']\n",
    "        if any(w in word for w in undercount_word_part):\n",
    "            syllables += 1\n",
    "        elif any(w in word for w in overcount_word_part):\n",
    "            syllables -= 1\n",
    "        yield None, (word, syllables)\n",
    "    \n",
    "    def reducer_syllable_sorter(self, _, syllable_word_pair):\n",
    "        #sorts the word, syllable list alphabetically, then it sorts the tuples by the second element\n",
    "        #which is the syllable count for the word, returning the top ten.\n",
    "        for word, syllables in sorted(sorted(syllable_word_pair), key=itemgetter(1), reverse=True)[:10]:\n",
    "                yield (word, syllables)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRWordSyllableCounter.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/syllables_count.jovyan.20200707.185013.850822\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/syllables_count.jovyan.20200707.185013.850822/output\n",
      "Streaming final output from /tmp/syllables_count.jovyan.20200707.185013.850822/output...\n",
      "\"incommunicability\"\t8\n",
      "\"materialistically\"\t8\n",
      "\"overcapitalization\"\t8\n",
      "\"apologetically\"\t7\n",
      "\"appreciatively\"\t7\n",
      "\"artificiality\"\t7\n",
      "\"autobiographical\"\t7\n",
      "\"characteristically\"\t7\n",
      "\"cosmopolitanism\"\t7\n",
      "\"enthusiastically\"\t7\n",
      "Removing temp directory /tmp/syllables_count.jovyan.20200707.185013.850822...\n",
      "10.054217338562012\n"
     ]
    }
   ],
   "source": [
    "!python syllables_count.py data/gutenberg/t5.churchill.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Again, temporary directories are created for processing.<br>\n",
    "> __The output is:__<br>\n",
    "> ```\"incommunicability\"\t8\n",
    "> \"materialistically\"\t8\n",
    "> \"overcapitalization\"\t8\n",
    "> \"apologetically\"\t7\n",
    "> \"appreciatively\"\t7     <- this one is miscounted\n",
    "> \"artificiality\"\t7\n",
    "> \"autobiographical\"\t7\n",
    "> \"characteristically\"\t7\n",
    "> \"cosmopolitanism\"\t7\n",
    "> \"enthusiastically\"\t7```<br>\n",
    "> Each of the above is a tuple with a word followed by the number of syllables in the word.  Within a given syllable about, words are ordered alphabetically.  After the temporary directory is closed, the time for the function is displayed in seconds (10.05 seconds for my last run)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
